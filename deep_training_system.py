#!/usr/bin/env python3
"""
üß† DEEP TRAINING SYSTEM - Training s√¢u v·ªõi d·ªØ li·ªáu chuy√™n s√¢u
T·∫°o v√† training v·ªõi h√†ng ngh√¨n Q&A pairs chuy√™n s√¢u v·ªÅ PC gaming
"""

import sqlite3
import json
import time
import random
from datetime import datetime
from typing import List, Dict, Tuple
import logging

import torch
import torch.nn.functional as F
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from datasets import Dataset
import requests

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DeepTrainingDataGenerator:
    """T·∫°o d·ªØ li·ªáu training s√¢u cho PC gaming AI"""
    
    def __init__(self, db_path="conversations.db"):
        self.db_path = db_path
        
        # PC Gaming knowledge base - DEEP DATA
        self.deep_pc_knowledge = {
            "cpu_data": [
                {
                    "name": "Intel Core i7-13700F",
                    "specs": "16 cores (8P+8E), 24 threads, 3.4-5.2GHz",
                    "price": "9,500,000",
                    "gaming_performance": "Excellent cho gaming 1440p/4K",
                    "pros": ["Hi·ªáu nƒÉng cao", "ƒêa nhi·ªám t·ªët", "Ti·∫øt ki·ªám ƒëi·ªán"],
                    "cons": ["Gi√° cao", "C·∫ßn t·∫£n nhi·ªát t·ªët"],
                    "use_cases": ["Gaming cao c·∫•p", "Streaming", "Content creation"]
                },
                {
                    "name": "AMD Ryzen 7 7700X",
                    "specs": "8 cores, 16 threads, 4.5-5.4GHz",
                    "price": "8,800,000",
                    "gaming_performance": "Tuy·ªát v·ªùi cho gaming, hi·ªáu nƒÉng/gi√° t·ªët",
                    "pros": ["Hi·ªáu nƒÉng/gi√° t·ªët", "Ti·∫øt ki·ªám ƒëi·ªán", "Overclock t·ªët"],
                    "cons": ["C·∫ßn RAM DDR5", "Gi√° mainboard cao"],
                    "use_cases": ["Gaming", "Productivity", "Budget high-end"]
                },
                {
                    "name": "Intel Core i5-13400F",
                    "specs": "10 cores (6P+4E), 16 threads, 2.5-4.6GHz",
                    "price": "4,500,000",
                    "gaming_performance": "R·∫•t t·ªët cho gaming 1080p/1440p",
                    "pros": ["Gi√° t·ªët", "Hi·ªáu nƒÉng ·ªïn", "T∆∞∆°ng th√≠ch r·ªông"],
                    "cons": ["√çt core h∆°n i7", "Kh√¥ng c√≥ iGPU"],
                    "use_cases": ["Gaming mainstream", "Office", "Budget build"]
                }
            ],
            
            "gpu_data": [
                {
                    "name": "RTX 4070",
                    "specs": "12GB GDDR6X, 2610MHz boost",
                    "price": "14,500,000",
                    "gaming_performance": "1440p Ultra 60+ FPS, 4K Medium-High",
                    "ray_tracing": "Excellent v·ªõi DLSS 3",
                    "vram": "12GB ƒë·ªß cho gaming hi·ªán t·∫°i v√† t∆∞∆°ng lai",
                    "power": "200W TGP",
                    "pros": ["VRAM 12GB", "DLSS 3", "Hi·ªáu nƒÉng t·ªët"],
                    "cons": ["Gi√° cao", "C·∫ßn PSU 650W+"],
                    "use_cases": ["Gaming 1440p", "Content creation", "Ray tracing"]
                },
                {
                    "name": "RTX 4060 Ti",
                    "specs": "16GB GDDR6, 2540MHz boost",
                    "price": "10,500,000",
                    "gaming_performance": "1440p High 60+ FPS, 1080p Ultra",
                    "ray_tracing": "Good v·ªõi DLSS",
                    "vram": "16GB version t·ªët cho t∆∞∆°ng lai",
                    "power": "165W TGP",
                    "pros": ["VRAM 16GB", "Ti·∫øt ki·ªám ƒëi·ªán", "Gi√° h·ª£p l√Ω"],
                    "cons": ["Bus 128-bit", "Hi·ªáu nƒÉng 1440p h·∫°n ch·∫ø"],
                    "use_cases": ["Gaming 1080p/1440p", "Budget high-end"]
                },
                {
                    "name": "RTX 4060",
                    "specs": "8GB GDDR6, 2460MHz boost",
                    "price": "8,200,000",
                    "gaming_performance": "1080p Ultra 60+ FPS, 1440p Medium-High",
                    "ray_tracing": "Decent v·ªõi DLSS",
                    "vram": "8GB ƒë·ªß cho 1080p gaming",
                    "power": "115W TGP",
                    "pros": ["Ti·∫øt ki·ªám ƒëi·ªán", "Gi√° t·ªët", "Compact"],
                    "cons": ["VRAM 8GB h·∫°n ch·∫ø", "Hi·ªáu nƒÉng 1440p th·∫•p"],
                    "use_cases": ["Gaming 1080p", "Budget gaming", "SFF builds"]
                }
            ],
            
            "ram_data": [
                {
                    "type": "DDR4-3200 16GB",
                    "price": "1,200,000",
                    "performance": "ƒê·ªß cho gaming hi·ªán t·∫°i",
                    "compatibility": "T∆∞∆°ng th√≠ch r·ªông v·ªõi Intel/AMD",
                    "pros": ["Gi√° r·∫ª", "T∆∞∆°ng th√≠ch t·ªët", "·ªîn ƒë·ªãnh"],
                    "cons": ["Ch·∫≠m h∆°n DDR5", "Kh√¥ng future-proof"],
                    "use_cases": ["Budget builds", "Upgrade c≈©", "Gaming 1080p"]
                },
                {
                    "type": "DDR5-5600 32GB",
                    "price": "3,800,000",
                    "performance": "Excellent cho gaming v√† productivity",
                    "compatibility": "Intel 12th gen+, AMD Ryzen 7000+",
                    "pros": ["T·ªëc ƒë·ªô cao", "Future-proof", "ƒêa nhi·ªám t·ªët"],
                    "cons": ["Gi√° cao", "T∆∞∆°ng th√≠ch h·∫°n ch·∫ø"],
                    "use_cases": ["High-end gaming", "Content creation", "Future builds"]
                }
            ],
            
            "storage_data": [
                {
                    "type": "SSD NVMe PCIe 4.0 1TB",
                    "price": "2,200,000",
                    "performance": "7000+ MB/s read, game load c·ª±c nhanh",
                    "pros": ["T·ªëc ƒë·ªô cao", "Future-proof", "Game load nhanh"],
                    "cons": ["Gi√° cao h∆°n PCIe 3.0", "C·∫ßn mainboard h·ªó tr·ª£"],
                    "use_cases": ["Gaming", "OS drive", "High-end builds"]
                },
                {
                    "type": "SSD SATA 1TB",
                    "price": "1,500,000",
                    "performance": "550 MB/s, ƒë·ªß cho gaming",
                    "pros": ["Gi√° t·ªët", "T∆∞∆°ng th√≠ch r·ªông", "ƒê√°ng tin c·∫≠y"],
                    "cons": ["Ch·∫≠m h∆°n NVMe", "C·∫ßn c√°p SATA"],
                    "use_cases": ["Budget builds", "Secondary storage", "Upgrade c≈©"]
                }
            ]
        }
        
        # Build configurations
        self.build_configs = {
            "budget_15m": {
                "cpu": "i5-13400F",
                "gpu": "RTX 4060",
                "ram": "DDR4-3200 16GB",
                "storage": "SSD SATA 500GB",
                "total": "15,000,000",
                "performance": "1080p Ultra gaming",
                "use_case": "Gaming entry-level"
            },
            "mid_range_25m": {
                "cpu": "i7-13700F",
                "gpu": "RTX 4060 Ti",
                "ram": "DDR4-3200 32GB",
                "storage": "SSD NVMe 1TB",
                "total": "25,000,000",
                "performance": "1440p High gaming",
                "use_case": "Gaming mainstream"
            },
            "high_end_35m": {
                "cpu": "i7-13700F",
                "gpu": "RTX 4070",
                "ram": "DDR5-5600 32GB",
                "storage": "SSD NVMe PCIe 4.0 1TB",
                "total": "35,000,000",
                "performance": "1440p Ultra, 4K Medium gaming",
                "use_case": "Gaming cao c·∫•p"
            }
        }
    
    def generate_deep_qa_pairs(self) -> List[Tuple[str, str]]:
        """Generate h√†ng ngh√¨n Q&A pairs chuy√™n s√¢u"""
        qa_pairs = []
        
        # 1. CPU Q&A
        for cpu in self.deep_pc_knowledge["cpu_data"]:
            qa_pairs.extend([
                (f"CPU {cpu['name']} c√≥ t·ªët kh√¥ng?", 
                 f"{cpu['name']} {cpu['gaming_performance']}. Specs: {cpu['specs']}. Gi√°: {cpu['price']} VND. ∆Øu ƒëi·ªÉm: {', '.join(cpu['pros'])}. Ph√π h·ª£p cho: {', '.join(cpu['use_cases'])}."),
                
                (f"Gi√° {cpu['name']} bao nhi√™u?",
                 f"{cpu['name']} hi·ªán t·∫°i gi√° kho·∫£ng {cpu['price']} VND. {cpu['gaming_performance']}. ƒê√¢y l√† l·ª±a ch·ªçn t·ªët cho {', '.join(cpu['use_cases'])}."),
                
                (f"Specs {cpu['name']} nh∆∞ th·∫ø n√†o?",
                 f"{cpu['name']} c√≥ {cpu['specs']}. {cpu['gaming_performance']}. ∆Øu ƒëi·ªÉm: {', '.join(cpu['pros'])}. Nh∆∞·ª£c ƒëi·ªÉm: {', '.join(cpu['cons'])}."),
                
                (f"{cpu['name']} gaming ƒë∆∞·ª£c kh√¥ng?",
                 f"C√≥! {cpu['name']} {cpu['gaming_performance']}. V·ªõi {cpu['specs']}, CPU n√†y ph√π h·ª£p cho {', '.join(cpu['use_cases'])}. Gi√° {cpu['price']} VND."),
                
                (f"N√™n mua {cpu['name']} kh√¥ng?",
                 f"N√™n! {cpu['name']} l√† l·ª±a ch·ªçn t·ªët v·ªõi {cpu['gaming_performance']}. ∆Øu ƒëi·ªÉm: {', '.join(cpu['pros'])}. Ph√π h·ª£p n·∫øu b·∫°n c·∫ßn {', '.join(cpu['use_cases'])}."),
            ])
        
        # 2. GPU Q&A
        for gpu in self.deep_pc_knowledge["gpu_data"]:
            qa_pairs.extend([
                (f"GPU {gpu['name']} c√≥ t·ªët kh√¥ng?",
                 f"{gpu['name']} r·∫•t t·ªët! {gpu['gaming_performance']}. VRAM: {gpu['vram']}, TGP: {gpu['power']}. Ray tracing: {gpu['ray_tracing']}. Gi√°: {gpu['price']} VND."),
                
                (f"RTX {gpu['name'].split()[-1]} gaming ƒë∆∞·ª£c kh√¥ng?",
                 f"ƒê∆∞·ª£c! {gpu['name']} {gpu['gaming_performance']}. {gpu['ray_tracing']}. ∆Øu ƒëi·ªÉm: {', '.join(gpu['pros'])}. Ph√π h·ª£p cho: {', '.join(gpu['use_cases'])}."),
                
                (f"Gi√° {gpu['name']} bao nhi√™u?",
                 f"{gpu['name']} gi√° kho·∫£ng {gpu['price']} VND. V·ªõi hi·ªáu nƒÉng {gpu['gaming_performance']}, ƒë√¢y l√† l·ª±a ch·ªçn t·ªët cho {', '.join(gpu['use_cases'])}."),
                
                (f"{gpu['name']} ch∆°i game 1440p ƒë∆∞·ª£c kh√¥ng?",
                 f"ƒê∆∞·ª£c! {gpu['name']} {gpu['gaming_performance']}. VRAM {gpu['vram']} ƒë·ªß cho gaming 1440p. {gpu['ray_tracing']}."),
                
                (f"So s√°nh {gpu['name']} v·ªõi card kh√°c?",
                 f"{gpu['name']}: {gpu['gaming_performance']}. ∆Øu ƒëi·ªÉm: {', '.join(gpu['pros'])}. Nh∆∞·ª£c ƒëi·ªÉm: {', '.join(gpu['cons'])}. TGP: {gpu['power']}."),
            ])
        
        # 3. Build Q&A
        for build_name, config in self.build_configs.items():
            budget = config['total']
            qa_pairs.extend([
                (f"Build PC {budget} ƒë∆∞·ª£c g√¨?",
                 f"PC {budget}: CPU {config['cpu']}, GPU {config['gpu']}, RAM {config['ram']}, SSD {config['storage']}. Hi·ªáu nƒÉng: {config['performance']}. Ph√π h·ª£p cho: {config['use_case']}."),
                
                (f"C·∫•u h√¨nh PC gaming {budget}?",
                 f"G·ª£i √Ω PC {budget}: {config['cpu']} + {config['gpu']} + {config['ram']} + {config['storage']}. Ch∆°i game {config['performance']}. T·ªïng: {config['total']} VND."),
                
                (f"PC {budget} ch∆°i game ƒë∆∞·ª£c kh√¥ng?",
                 f"ƒê∆∞·ª£c! PC {budget} v·ªõi {config['cpu']} + {config['gpu']} c√≥ th·ªÉ {config['performance']}. C·∫•u h√¨nh n√†y ph√π h·ª£p cho {config['use_case']}."),
            ])
        
        # 4. Comparison Q&A
        qa_pairs.extend([
            ("Intel vs AMD CPU n√†o t·ªët h∆°n?",
             "Intel: Hi·ªáu nƒÉng gaming cao, t∆∞∆°ng th√≠ch t·ªët. AMD: Gi√°/hi·ªáu nƒÉng t·ªët, ti·∫øt ki·ªám ƒëi·ªán. Intel t·ªët cho gaming thu·∫ßn, AMD t·ªët cho ƒëa nhi·ªám."),
            
            ("RTX vs GTX kh√°c nhau g√¨?",
             "RTX c√≥ Ray Tracing v√† DLSS, hi·ªáu nƒÉng cao h∆°n. GTX gi√° r·∫ª h∆°n nh∆∞ng kh√¥ng c√≥ RT/DLSS. RTX ph√π h·ª£p gaming cao c·∫•p, GTX cho budget."),
            
            ("DDR4 vs DDR5 n√™n ch·ªçn g√¨?",
             "DDR5: Nhanh h∆°n, future-proof nh∆∞ng ƒë·∫Øt. DDR4: R·∫ª, t∆∞∆°ng th√≠ch r·ªông. Ch·ªçn DDR5 cho build m·ªõi cao c·∫•p, DDR4 cho budget/upgrade."),
            
            ("SSD vs HDD n√™n d√πng g√¨?",
             "SSD: Nhanh, game load nhanh, b·ªÅn. HDD: R·∫ª, dung l∆∞·ª£ng l·ªõn. D√πng SSD cho OS/game, HDD cho l∆∞u tr·ªØ."),
        ])
        
        # 5. Troubleshooting Q&A
        qa_pairs.extend([
            ("PC kh√¥ng kh·ªüi ƒë·ªông ƒë∆∞·ª£c?",
             "Ki·ªÉm tra: 1) Ngu·ªìn ƒëi·ªán, 2) RAM l·∫Øp ch·∫∑t, 3) C√°p 24pin + 8pin CPU, 4) GPU l·∫Øp ch·∫∑t, 5) Monitor c·∫Øm ƒë√∫ng c·ªïng GPU."),
            
            ("Game b·ªã lag, gi·∫≠t?",
             "Nguy√™n nh√¢n: 1) GPU y·∫øu, 2) RAM kh√¥ng ƒë·ªß, 3) CPU bottleneck, 4) Nhi·ªát ƒë·ªô cao, 5) Driver c≈©. Ki·ªÉm tra t·ª´ng c√°i."),
            
            ("PC n√≥ng qu√°?",
             "Gi·∫£i ph√°p: 1) V·ªá sinh qu·∫°t, 2) Thay keo t·∫£n nhi·ªát, 3) Th√™m qu·∫°t case, 4) Ki·ªÉm tra airflow, 5) Undervolt CPU/GPU."),
            
            ("M√†n h√¨nh kh√¥ng c√≥ t√≠n hi·ªáu?",
             "Ki·ªÉm tra: 1) C√°p monitor, 2) C·∫Øm v√†o GPU kh√¥ng ph·∫£i mainboard, 3) RAM l·∫Øp ch·∫∑t, 4) GPU c√≥ ngu·ªìn, 5) Monitor b·∫≠t ƒë√∫ng input."),
        ])
        
        logger.info(f"Generated {len(qa_pairs)} deep Q&A pairs")
        return qa_pairs
    
    def save_deep_training_data(self, qa_pairs: List[Tuple[str, str]]):
        """Save deep training data to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        saved_count = 0
        for question, answer in qa_pairs:
            try:
                cursor.execute('''
                    INSERT INTO conversations (user_id, message, response, confidence, source)
                    VALUES (?, ?, ?, ?, ?)
                ''', ('deep_training', question, answer, 0.95, 'deep_generated'))
                saved_count += 1
            except Exception as e:
                logger.error(f"Error saving Q&A: {e}")
        
        conn.commit()
        conn.close()
        
        logger.info(f"üíæ Saved {saved_count} deep training pairs to database")
        return saved_count
    
    def generate_and_save_deep_data(self):
        """Generate v√† save t·∫•t c·∫£ deep training data"""
        logger.info("üß† Generating deep training data...")
        
        # Generate Q&A pairs
        qa_pairs = self.generate_deep_qa_pairs()
        
        # Save to database
        saved_count = self.save_deep_training_data(qa_pairs)
        
        logger.info(f"‚úÖ Deep training data generation complete: {saved_count} pairs")
        return saved_count

class DeepTrainingSystem:
    """Deep training system v·ªõi enhanced model"""
    
    def __init__(self, model_name="microsoft/DialoGPT-medium"):
        self.model_name = model_name
        self.db_path = "conversations.db"
        self.model_save_path = "./deep_trained_model"
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        logger.info(f"Using device: {self.device}")
        if torch.cuda.is_available():
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
    
    def load_model(self):
        """Load model for deep training"""
        logger.info(f"Loading {self.model_name} for deep training...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
        self.model.to(self.device)
        
        logger.info("‚úÖ Model loaded successfully")
    
    def get_deep_training_data(self) -> List[Tuple[str, str]]:
        """Get all training data for deep training"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Get all conversations including deep generated ones
        cursor.execute('''
            SELECT message, response FROM conversations 
            WHERE confidence > 0.7
            ORDER BY timestamp DESC
        ''')
        
        training_data = cursor.fetchall()
        conn.close()
        
        logger.info(f"üìä Retrieved {len(training_data)} training samples")
        return training_data
    
    def deep_train_model(self):
        """Perform deep training"""
        logger.info("üß† Starting DEEP TRAINING...")
        
        # Get training data
        training_data = self.get_deep_training_data()
        
        if len(training_data) < 10:
            logger.error("Not enough training data for deep training")
            return False
        
        # Prepare training texts
        texts = []
        for question, answer in training_data:
            text = f"{question}{self.tokenizer.eos_token}{answer}{self.tokenizer.eos_token}"
            texts.append(text)
        
        # Tokenize
        logger.info("üî§ Tokenizing training data...")
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        
        dataset = Dataset.from_dict({
            "input_ids": encodings["input_ids"],
            "attention_mask": encodings["attention_mask"]
        })
        
        # Training arguments for deep training
        training_args = TrainingArguments(
            output_dir=self.model_save_path,
            overwrite_output_dir=True,
            num_train_epochs=3,  # More epochs for deep training
            per_device_train_batch_size=2,  # Smaller batch for deeper training
            gradient_accumulation_steps=4,  # Accumulate gradients
            save_steps=50,
            save_total_limit=3,
            prediction_loss_only=True,
            learning_rate=3e-5,  # Lower learning rate for stability
            warmup_steps=100,
            logging_steps=10,
            fp16=torch.cuda.is_available(),
            dataloader_drop_last=True,
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=dataset,
        )
        
        # Deep training
        logger.info(f"üöÄ Deep training with {len(training_data)} samples...")
        trainer.train()
        
        # Save deep trained model
        trainer.save_model()
        self.tokenizer.save_pretrained(self.model_save_path)
        
        logger.info("‚úÖ DEEP TRAINING COMPLETED!")
        return True

def main():
    """Main function for deep training"""
    logger.info("üß† STARTING DEEP TRAINING SYSTEM...")
    
    # 1. Generate deep training data
    data_generator = DeepTrainingDataGenerator()
    data_generator.generate_and_save_deep_data()
    
    # 2. Perform deep training
    training_system = DeepTrainingSystem()
    training_system.load_model()
    training_system.deep_train_model()
    
    logger.info("üéâ DEEP TRAINING SYSTEM COMPLETE!")

if __name__ == "__main__":
    main()
